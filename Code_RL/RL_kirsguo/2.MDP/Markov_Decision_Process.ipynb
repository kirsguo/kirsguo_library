{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编程实践二 理解MRP、MDP以及相关知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![马尔科夫奖励过程示例](https://github.com/kirsguo/kirsguo_library/blob/master/Code_RL/RL_kirsguo/2.MDP/mrp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面代码定义学生马尔科夫奖励过程所需要的信息\n",
    "#### 其中状态集数量为7（两个字典双向映射），状态转移概率用7*7矩阵表示\n",
    "#### 折扣因子为gamma 奖励函数用reward表示，分别与状态对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 7\n",
    "# {\"0\": \"C1\", \"1\":\"C2\", \"2\":\"C3\", \"3\":\"Pass\", \"4\":\"Pub\", \"5\":\"FB\", \"6\":\"Sleep\"}\n",
    "i_to_n = {}\n",
    "i_to_n[\"0\"] = \"C1\"\n",
    "i_to_n[\"1\"] = \"C2\"\n",
    "i_to_n[\"2\"] = \"C3\"\n",
    "i_to_n[\"3\"] = \"Pass\"\n",
    "i_to_n[\"4\"] = \"Pub\"\n",
    "i_to_n[\"5\"] = \"FB\"\n",
    "i_to_n[\"6\"] = \"Sleep\"\n",
    "\n",
    "n_to_i = {}\n",
    "for i, name in zip(i_to_n.keys(), i_to_n.values()):\n",
    "    n_to_i[name] = int(i)\n",
    "    \n",
    "#   C1   C2   C3  Pass  Pub  FB  Sleep\n",
    "Pss = [\n",
    "   [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
    "   [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2],\n",
    "   [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0],\n",
    "   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "   [0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
    "   [0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0],\n",
    "   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "]\n",
    "Pss = np.array(Pss)\n",
    "rewards = [-2, -2, -2, 10, 1, -1, 0]\n",
    "gamma = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义以状态C1开始的马尔科夫链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = [\n",
    "    [\"C1\", \"C2\", \"C3\", \"Pass\", \"Sleep\"],\n",
    "    [\"C1\", \"FB\", \"FB\", \"C1\", \"C2\", \"Sleep\"],\n",
    "    [\"C1\", \"C2\", \"C3\", \"Pub\", \"C2\", \"C3\", \"Pass\", \"Sleep\"],\n",
    "    [\"C1\", \"FB\", \"FB\", \"C1\", \"C2\", \"C3\", \"Pub\", \"C1\", \"FB\",\n",
    "     \"FB\", \"FB\", \"C1\", \"C2\", \"C3\", \"Pub\", \"C2\", \"Sleep\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_return(start_index=0, \n",
    "                   chain=None, \n",
    "                   gamma=0.5) -> float:\n",
    "    '''\n",
    "    计算一个马尔科夫奖励过程中某状态的收获值\n",
    "    :param start_index: 要计算的状态在链中的位置\n",
    "    :param chain: 要计算的马尔科夫过程\n",
    "    :param gamma: 折扣因子\n",
    "    :return: 收获\n",
    "    '''\n",
    "    count, power,  = 0.0, 0\n",
    "    for i in range(start_index, len(chain)):\n",
    "        count += np.power(gamma, power) * rewards[n_to_i[chain[i]]]\n",
    "        power += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证上文定义最后一条马尔科夫链的起始状态收获值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.196044921875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_return(0, chains[3], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(Pss, rewards, gamma = 0.05):\n",
    "    '''\n",
    "    通过求解矩阵方程的形式直接计算状态的价值\n",
    "    :param Pss: 状态转移概率矩阵 shape(7, 7)\n",
    "    :param rewards: 即时奖励 list\n",
    "    :param gamma: 折扣因子\n",
    "    :return: values 各状态的价值\n",
    "    '''\n",
    "    \n",
    "    rewards = np.array(rewards).reshape((-1, 1))\n",
    "    values = np.dot(np.linalg.inv(np.eye(7, 7) - gamma * Pss), rewards)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 求解状态价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.54296219]\n [  1.4568013 ]\n [  4.32100594]\n [ 10.        ]\n [  0.80253065]\n [-22.54274676]\n [  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "values = compute_value(Pss, rewards, gamma = 0.999999)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![马尔科夫决策过程示例](https://github.com/kirsguo/kirsguo_library/blob/master/Code_RL/RL_kirsguo/2.MDP/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先我们先定义一些基本的操作，我们将基于行为的状态转移概率矩阵以及基于行为的奖励函数用字典来存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Jupyter_Notebook_workplace/utils.py\n",
    "# 辅助函数\n",
    "def str_key(*args):\n",
    "    '''将参数用\"_\"连接起来作为字典的键，需注意参数本身可能会是tuple或者list型，\n",
    "    比如类似((a,b,c),d)的形式。\n",
    "    '''\n",
    "    new_arg = []\n",
    "    for arg in args:\n",
    "        if type(arg) in [tuple, list]:\n",
    "            new_arg += [str(i) for i in arg]\n",
    "        else:\n",
    "            new_arg.append(str(arg))\n",
    "    return \"_\".join(new_arg)\n",
    "\n",
    "\n",
    "def set_dict(target_dict, value, *args):\n",
    "    target_dict[str_key(*args)] = value\n",
    "\n",
    "\n",
    "def set_prob(P, s, a, s1, p=1.0):  # 设置概率字典,默认为1\n",
    "    set_dict(P, p, s, a, s1)\n",
    "\n",
    "\n",
    "def get_prob(P, s, a, s1):  # 获取概率值\n",
    "    return P.get(str_key(s, a, s1), 0)\n",
    "\n",
    "\n",
    "def set_reward(R, s, a, r):  # 设置奖励字典\n",
    "    set_dict(R, r, s, a)\n",
    "\n",
    "\n",
    "def get_reward(R, s, a):  # 获取奖励值\n",
    "    return R.get(str_key(s, a), 0)\n",
    "\n",
    "\n",
    "def display_dict(target_dict):  # 显示字典内容\n",
    "    for key in target_dict.keys():\n",
    "        print(\"{}:　{:.2f}\".format(key, target_dict[key]))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# 辅助方法\n",
    "def set_value(V, s, v):  # 设置价值字典\n",
    "    set_dict(V, v, s)\n",
    "\n",
    "\n",
    "def get_value(V, s):  # 获取价值值\n",
    "    return V.get(str_key(s), 0)\n",
    "\n",
    "\n",
    "def set_pi(Pi, s, a, p=0.5):  # 设置策略字典\n",
    "    set_dict(Pi, p, s, a)\n",
    "\n",
    "\n",
    "def get_pi(Pi, s, a):  # 获取策略（概率）值\n",
    "    return Pi.get(str_key(s, a), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建学生马尔科夫决策过程\n",
    "S = ['浏览手机中', '第一节课', '第二节课', '第三节课', '休息中']\n",
    "A = ['浏览手机', '学习', '离开浏览', '泡吧', '退出学习']\n",
    "R = {} # 基于状态和行为的奖励Rsa\n",
    "P = {} # 基于行为的状态转移概率Pss'a\n",
    "gamma = 1.0 # 衰减因子\n",
    "\n",
    "set_prob(P, S[0], A[0], S[0])  # 浏览手机中 - 浏览手机 -> 浏览手机中\n",
    "set_prob(P, S[0], A[2], S[1])  # 浏览手机中 - 离开浏览 -> 第一节课\n",
    "set_prob(P, S[1], A[0], S[0])  # 第一节课 - 浏览手机 -> 浏览手机中\n",
    "set_prob(P, S[1], A[1], S[2])  # 第一节课 - 学习 -> 第二节课\n",
    "set_prob(P, S[2], A[1], S[3])  # 第二节课 - 学习 -> 第三节课\n",
    "set_prob(P, S[2], A[4], S[4])  # 第二节课 - 退出学习 -> 退出休息\n",
    "set_prob(P, S[3], A[1], S[4])  # 第三节课 - 学习 -> 退出休息\n",
    "set_prob(P, S[3], A[3], S[1], p=0.2)  # 第三节课 - 泡吧 -> 第一节课\n",
    "set_prob(P, S[3], A[3], S[2], p=0.4)  # 第三节课 - 泡吧 -> 第一节课\n",
    "set_prob(P, S[3], A[3], S[3], p=0.4)  # 第三节课 - 泡吧 -> 第一节课\n",
    "\n",
    "set_reward(R, S[0], A[0], -1)  # 浏览手机中 - 浏览手机 -> -1\n",
    "set_reward(R, S[0], A[2],  0)  # 浏览手机中 - 离开浏览 -> 0\n",
    "set_reward(R, S[1], A[0], -1)  # 第一节课 - 浏览手机 -> -1\n",
    "set_reward(R, S[1], A[1], -2)  # 第一节课 - 学习 -> -2\n",
    "set_reward(R, S[2], A[1], -2)  # 第二节课 - 学习 -> -2\n",
    "set_reward(R, S[2], A[4],  0)  # 第二节课 - 退出学习 -> 0\n",
    "set_reward(R, S[3], A[1], 10)  # 第三节课 - 学习 -> 10\n",
    "set_reward(R, S[3], A[3], +1)  # 第三节课 - 泡吧 -> -1\n",
    "\n",
    "MDP = (S, A, R, P, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以上就是我们通过代码初始的一个马尔科夫决策过程的模型，接下来我们可以调用display_dict方法来验证一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----基于行为的状态转移概率字典（矩阵）信息:----\n浏览手机中_浏览手机_浏览手机中:　1.00\n浏览手机中_离开浏览_第一节课:　1.00\n第一节课_浏览手机_浏览手机中:　1.00\n第一节课_学习_第二节课:　1.00\n第二节课_学习_第三节课:　1.00\n第二节课_退出学习_休息中:　1.00\n第三节课_学习_休息中:　1.00\n第三节课_泡吧_第一节课:　0.20\n第三节课_泡吧_第二节课:　0.40\n第三节课_泡吧_第三节课:　0.40\n\n----基于状态和行为奖励字典（函数）信息:----\n浏览手机中_浏览手机:　-1.00\n浏览手机中_离开浏览:　0.00\n第一节课_浏览手机:　-1.00\n第一节课_学习:　-2.00\n第二节课_学习:　-2.00\n第二节课_退出学习:　0.00\n第三节课_学习:　10.00\n第三节课_泡吧:　1.00\n\n"
     ]
    }
   ],
   "source": [
    "print(\"----基于行为的状态转移概率字典（矩阵）信息:----\")\n",
    "display_dict(P)\n",
    "print(\"----基于状态和行为奖励字典（函数）信息:----\")\n",
    "display_dict(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接下来我们将进行策略评估，在这里我们采用均一随机策略，将某一状态下采取所有行为的可能性概率相等。这里用Pi来表示策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----策略概率字典（矩阵）信息:----\n浏览手机中_浏览手机:　0.50\n浏览手机中_离开浏览:　0.50\n第一节课_浏览手机:　0.50\n第一节课_学习:　0.50\n第二节课_学习:　0.50\n第二节课_退出学习:　0.50\n第三节课_学习:　0.50\n第三节课_泡吧:　0.50\n\n----状态转移概率字典（矩阵）信息:----\n\n"
     ]
    }
   ],
   "source": [
    "# S = ['浏览手机中','第一节课','第二节课','第三节课','休息中']\n",
    "# A = ['继续浏览','学习','离开浏览','泡吧','退出学习']\n",
    "# 设置行为策略：pi(a|.) = 0.5\n",
    "Pi = {}\n",
    "set_pi(Pi, S[0], A[0], 0.5) # 浏览手机中 - 浏览手机\n",
    "set_pi(Pi, S[0], A[2], 0.5) # 浏览手机中 - 离开浏览\n",
    "set_pi(Pi, S[1], A[0], 0.5) # 第一节课 - 浏览手机\n",
    "set_pi(Pi, S[1], A[1], 0.5) # 第一节课 - 学习\n",
    "set_pi(Pi, S[2], A[1], 0.5) # 第二节课 - 学习\n",
    "set_pi(Pi, S[2], A[4], 0.5) # 第二节课 - 退出学习\n",
    "set_pi(Pi, S[3], A[1], 0.5) # 第三节课 - 学习\n",
    "set_pi(Pi, S[3], A[3], 0.5) # 第三节课 - 泡吧\n",
    "\n",
    "print(\"----策略概率字典（矩阵）信息:----\")\n",
    "display_dict(Pi)\n",
    "# 初始时价值为空，访问时会返回0\n",
    "print(\"----状态转移概率字典（矩阵）信息:----\")\n",
    "V = {}\n",
    "display_dict(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面给出行为价值函数q(s,a)以及价值函数v_pi(s)的计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q(MDP, V, s, a):\n",
    "    '''\n",
    "    根据给定的MDP，价值函数V，计算行为价值函数q(s,a)\n",
    "    '''\n",
    "    S, A, R, P, gamma = MDP\n",
    "    q_sa = 0\n",
    "    for s_prime in S:\n",
    "        q_sa += get_prob(P, s, a, s_prime) * get_value(V, s_prime)\n",
    "    q_sa = get_reward(R, s, a) + gamma * q_sa\n",
    "    return q_sa\n",
    "\n",
    "\n",
    "def compute_v(MDP, V, Pi, s):\n",
    "    '''\n",
    "    给定MDP下依据某一策略Pi和当前状态价值函数V计算价值函数v_pi_s\n",
    "    '''\n",
    "    S, A, R, P, gamma = MDP\n",
    "    v_s = 0\n",
    "    for a in A:\n",
    "        v_s += get_pi(Pi, s, a) * compute_q(MDP, V, s, a)\n",
    "    return v_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于贝尔曼最优方程的理解，编程实现需要用到迭代方面的知识，我们暂时不考虑",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
